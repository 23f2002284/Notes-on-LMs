{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9648d74a",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "# Sinusodial Positional Embeddings\n",
    "[> Sinusodial Positional encoding code](./sinusodial_PE.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11fe40",
   "metadata": {},
   "source": [
    "### 1. Why embedding dimension have to be even ?\n",
    "- The embedding dimension must be even because the sinusoidal encoding alternates between sine and cosine functions. For each position, even indices use sine and odd indices use cosine.\n",
    "- If the embedding dimension were odd, there would be no way to alternate between sine and cosine functions for all positions. and we wouldn't have matching pairs\n",
    "```python\n",
    "if emb_dim % 2 != 0:\n",
    "    raise ValueError(f\"emb_dim must be even, but got{emb_dim}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2d745",
   "metadata": {},
   "source": [
    "### 2. what is dropout ? why we need dropout here ?\n",
    "- Dropout is a regularization technique that randomly sets a fraction of input units to 0 during training to prevent overfitting.\n",
    "-  it's applied to the combined input embeddings and positional encodings to help the model generalize better by preventing `co-adaptation of features`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a344eac",
   "metadata": {},
   "source": [
    "### 2.1 Define co-adaption of features ?\n",
    "Co-adaptation occurs when certain neurons in a neural network rely too heavily on specific other neurons, making the network less robust and more prone to overfitting.\n",
    "> For example, if neuron A always activates with neuron B, the network might not learn meaningful patterns independently.\n",
    "\n",
    "**Why it's problematic:**\n",
    "- Reduces model generalization\n",
    "- Makes the network sensitive to specific training data patterns\n",
    "- Can lead to overfitting\n",
    "\n",
    "**How dropout helps:**\n",
    "- Randomly drops out (sets to zero) a fraction of the input units during training \n",
    "- Randomly deactivates neurons during training\n",
    "- Encourages the network to learn more robust features\n",
    "- Prevents co-adaptation by forcing neurons to learn independently\n",
    "- Prevents neurons from relying too much on specific activations\n",
    "- Acts as an ensemble method by training different subnetworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d4680",
   "metadata": {},
   "source": [
    "### 3. why we are having position as arange and then unsqueeze ?\n",
    "```python\n",
    "# create a 1D  tensor of positions [0, 1, ..., seq_len-1]\n",
    "torch.arange(seq_len)\n",
    "# unsqueeze to make it a 2D tensor of shape [seq_len, 1]\n",
    "# like [[0], [1], ..., [seq_len-1]]\n",
    "torch.arange(seq_len).unsqueeze(1)\n",
    "```\n",
    "- This is done to enable broadcasting when multiplying with `div_term` which has shape `[emb_dim//2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66c9f4",
   "metadata": {},
   "source": [
    "### 4. why we are using register buffer ?\n",
    "```python\n",
    "self.register_buffer('pe', pe.unsqueeze(0))\n",
    "```\n",
    "- `register_buffer` is used to register a buffer(a tensor) (e.g. `pos_emb`) that is not a parameter of the module but should be part of the module's state.\n",
    "    -  Moved to the same device as the module's parameters\n",
    "    -  Saved in the state_dict for saving and loading\n",
    "    - Not considered a trainable parameter (unlike `nn.Parameter`)\n",
    "    - The positional encodings are constant and don't need gradients, so they're stored as a buffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c09fcb",
   "metadata": {},
   "source": [
    "### 5. what is forward doing here ? \n",
    "1. Takes input `x` of shape (batch_size, seq_len, emb_dim)\n",
    "2. Adds the positional encoding to the input tensor.\n",
    "3. Slices the positional encoding to match the input sequence length.\n",
    "4. applies dropout to the result\n",
    "5. Returns the position aware embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d6f08",
   "metadata": {},
   "source": [
    "### Extra: why did we use nn.Module in the class ?\n",
    "nn.Module is the base class for all neural network modules in PyTorch. \n",
    "\n",
    "1. Parameter Management\n",
    "    - Tracks all `nn.Parameter` objects\n",
    "    - Enables automatic differentiation\n",
    "    - Handles moving parameters to GPU/CPU\n",
    "\n",
    "2. State Management\n",
    "    - Maintains model state (train/eval modes)\n",
    "    - Handles saving/loading model state ( model persistence )\n",
    "    - Manage buffer (register) \n",
    "3. Module Composition\n",
    "    - Enables building complex architecture\n",
    "    - Supports nested modules\n",
    "    - Provide `to()`, `train()`, `eval()`, `parameters()`, `state_dict()`, `load_state_dict()` methods\n",
    "4. Forward Hooks\n",
    "    - allow to insert custom operations at any point in the forward pass\n",
    "    - useful for debugging, visualization, and custom training \n",
    "5. Integrate with PyTorch's autograd engine\n",
    "    - Enable custom gradient computation\n",
    "    - Support custom backward passes\n",
    "    - compatible with DataLoader\n",
    "\n",
    "in our case `nn.Module` help us to\n",
    "1. Register the positional encoding buffer\n",
    "2. Use the module in a PyTorch model\n",
    "Move all tensors to the correct device (CPU/GPU) automatically\n",
    "3. Save/load the model's state including the positional encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89e4fc",
   "metadata": {},
   "source": [
    "# Learned embeddings \n",
    "[> Learned Positional encoding code](./learned_PE.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bca637",
   "metadata": {},
   "source": [
    "### 1. why we are using embedding layer ? not linear or parameter layer ?\n",
    "- an embedding layer is a lookup table where the key is the position index and the value is a learned vector of size emb_dim.\n",
    "    - we use it for position encoding, word embeddings or any discrete feature mapping\n",
    "- a linear layer is a layer that performs a linear transformation of the input tensor.\n",
    "    - it simply applies a learned parameter matrix to the input tensor.\n",
    "    -  we use it for feature transformation, non-linearities, etc.\n",
    "- a parameter layer is a layer that has a parameter matrix that is learned during training.\n",
    "    - we use it when we need direct access to learnable parameters with custom update logic\n",
    "    - we use it when we need to share parameters across different parts of the model\n",
    "\n",
    "# Positional Embeddings: Implementation and Comparison\n",
    "\n",
    "## 1. Core Implementation Choices\n",
    "\n",
    "### 1.1 Embedding Layer\n",
    "- **Lookup Efficiency**: O(1) complexity for direct position vector access\n",
    "- **Memory Layout**: Optimized for sparse lookups\n",
    "- **Gradient Flow**: Clean, direct gradient flow to position vectors\n",
    "- **Parameter Storage**: Stores exactly one vector per position\n",
    "\n",
    "### 1.2 Why Not Linear Layer?\n",
    "- **Complexity**: O(nÂ²) due to matrix multiplication\n",
    "- **Memory**: Requires weight matrix of size [seq_len, emb_dim]\n",
    "- **Inefficiency**: Processes all positions even if not used\n",
    "- **Gradient Flow**: Unnecessary computation through weight matrix\n",
    "\n",
    "### 1.3 Why Not Parameter Layer?\n",
    "- **Memory**: Similar to embedding but less efficient\n",
    "- **Optimization**: No specialized sparse lookup optimization\n",
    "- **Implementation**: Less optimized backward pass\n",
    "- **Flexibility**: Harder to extend with relative positions\n",
    "\n",
    "## 2. Key Features\n",
    "\n",
    "### 2.1 Scaling Factor\n",
    "- **Purpose**: Controls magnitude of positional embeddings\n",
    "- **Benefit**: Prevents dominance over input embeddings\n",
    "- **Implementation**: Simple multiplicative scaling\n",
    "- **Default**: 1.0 (no scaling)\n",
    "\n",
    "### 2.2 Layer Normalization\n",
    "- **Purpose**: Stabilizes training\n",
    "- **Benefit**: Normalizes combined embeddings\n",
    "- **Placement**: After position addition\n",
    "- **Impact**: Improves gradient flow\n",
    "\n",
    "### 2.3 Relative Positions\n",
    "- **Purpose**: Captures position relationships\n",
    "- **Benefit**: Better for tasks where relative positions matter\n",
    "- **Implementation**: Additional learnable bias terms\n",
    "- **Use Case**: When sequence order matters more than absolute position\n",
    "\n",
    "## 3. Performance Considerations\n",
    "\n",
    "### 3.1 Memory Efficiency\n",
    "- Embedding: Most efficient (one vector per position)\n",
    "- Linear: Least efficient (full weight matrix)\n",
    "- Parameter: Similar to embedding but less optimized\n",
    "\n",
    "### 3.2 Training Dynamics\n",
    "- **Sparse Updates**: Only updates used positions\n",
    "- **Gradient Flow**: Direct paths to position vectors\n",
    "- **Convergence**: Typically faster with embedding layers\n",
    "- **Stability**: Improved with proper scaling and normalization\n",
    "\n",
    "## 4. Best Practices\n",
    "\n",
    "1. **Always** use embedding layers for positional encodings\n",
    "2. **Consider** adding layer normalization for deeper networks\n",
    "3. **Use** relative positions for tasks where sequence relationships matter\n",
    "4. **Experiment** with scaling factors (start with 1.0)\n",
    "5. **Monitor** gradient norms to ensure stable training\n",
    "\n",
    "## 5. Example Usage\n",
    "\n",
    "```python\n",
    "# Basic usage\n",
    "pe = LearnedPositionalEmbedding(emb_dim=512)\n",
    "\n",
    "# With all features\n",
    "pe = LearnedPositionalEmbedding(\n",
    "    emb_dim=512,\n",
    "    dropout=0.1,\n",
    "    seq_len=1024,\n",
    "    scale=0.5,\n",
    "    use_ln=True,\n",
    "    relative=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ace96",
   "metadata": {},
   "source": [
    "### 2. why we register positions as buffer ?\n",
    "- Registered buffers are saved and loaded with the model's state dictionary => model maintain a state dictionary and thanks to nn.Moduel\n",
    "- Ensures consistent behavior when saving/loading the model\n",
    "- Automatically moves to the same device as the module's parameters => No need for manual .to(device) calls\n",
    "- Positions are fixed indices, not learnable parameters\n",
    "- More memory efficient than parameters since they don't store gradients => parameters stores gradient as well\n",
    "- Created once during initialization => that's why they are in `__init__()` => Avoids recreating the position tensor on every forward pass\n",
    "- Ensures consistent behavior across different runs\n",
    "\n",
    "Without it \n",
    "1. inefficient\n",
    "    - we would create a new tensor on every forward pass\n",
    "2. Device mismatch\n",
    "    - Could lead to \"tensors on different devices\" errors\n",
    "3. State Issues: Positions wouldn't be saved with the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec45f4e",
   "metadata": {},
   "source": [
    "### 3. how we used positional embedding in forward pass ?\n",
    "```python\n",
    "pos_embeddings = self.position_embedding(self.positions[:seq_len])\n",
    "\n",
    "x = x + pos_embeddings\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a002101",
   "metadata": {},
   "source": [
    "### 4. why are we returning with dropout ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846c836",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "[> RoPE code](./RoPE.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88077440",
   "metadata": {},
   "source": [
    "### 1. what is inverse frequency ? what is it's need ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2967d4",
   "metadata": {},
   "source": [
    "### 2. what are the actual practices done instead of precomputing the positions with arange ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424217d",
   "metadata": {},
   "source": [
    "### 3. what is outer product ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bcd57",
   "metadata": {},
   "source": [
    "### 4. what is einsum ? what this line implies ?\n",
    "```python\n",
    "freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75d42d",
   "metadata": {},
   "source": [
    "### 5. why we are concatenating frequencies to handle both sin and cos ? what is the pratice followed here ?\n",
    "```python\n",
    "emb = torch.cat((freqs, freqs), dim=-1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d7d1c",
   "metadata": {},
   "source": [
    "### 6. why we are registering buffer for cos and sin ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50541007",
   "metadata": {},
   "source": [
    "### 7. x_pairs significance in the code ?\n",
    "```python\n",
    "x_pairs = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd0bc5",
   "metadata": {},
   "source": [
    "### 8. why we took even and odd indexed features as real and imaginary respectively ?\n",
    "```python\n",
    "x1 = x_pairs[..., 0]  # Real part\n",
    "x2 = x_pairs[..., 1]  # Imaginary part\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad358906",
   "metadata": {},
   "source": [
    "### 9. why element wise multiplication instead of matrix multiplication ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6dd17d",
   "metadata": {},
   "source": [
    "### 10. why are returning both query and key ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379c813",
   "metadata": {},
   "source": [
    "# ALiBi\n",
    "[> ALiBi code](./Alibi_PE.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bceb8b",
   "metadata": {},
   "source": [
    "### 1. why we needed slopes and heads unlike others ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0f423",
   "metadata": {},
   "source": [
    "### 2. How we implemented the concepts from paper line by line ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c42fb",
   "metadata": {},
   "source": [
    "### 3. what is alibi_bias ? in significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1580ed4",
   "metadata": {},
   "source": [
    "### 4. difference between model's state and a trainable parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd94d61",
   "metadata": {},
   "source": [
    "### 5. How we are calculating the slopes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7939c",
   "metadata": {},
   "source": [
    "### 6. Significance of alibi bias to the attention scores ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe22be",
   "metadata": {},
   "source": [
    "### 7. why we are slicing the alibi bias ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f50795",
   "metadata": {},
   "source": [
    "### Extra: Practices for implementing the equation in sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec713a",
   "metadata": {},
   "source": [
    "# Comparison of Sinusodial, learned embeddings, RoPE, ALiBi\n",
    "| Property | Sinusoidal | Learned Embeddings | RoPE | ALiBi |\n",
    "|----------|------------|-------------------|------|-------|\n",
    "| **Type** | Deterministic | Learned | Hybrid (deterministic + learned) | Learned |\n",
    "| **Training** | Fixed, not trainable | Trainable | Partially trainable (applies rotation to learned queries/keys) | Trainable |\n",
    "| **Sequence Length** | Fixed maximum length | Fixed maximum length | Flexible, better generalization | Excellent extrapolation to longer sequences |\n",
    "| **Position Info** | Absolute positions | Absolute positions | Relative positions | Relative positions with bias |\n",
    "| **Computation** | Additive | Additive | Multiplicative (rotations) | Multiplicative (attention bias) |\n",
    "| **Memory** | Low | Medium | Medium | Low |\n",
    "| **Performance** | Good for short sequences | Better than sinusoidal with careful init | Excellent for relative positions | Best for long sequences |\n",
    "| **Training Speed** | Fastest | Slower (learned params) | Moderate | Fast (efficient bias) |\n",
    "| **Use Cases** | Early Transformers | Early BERT variants | LLaMA, GPT-Neo | BLOOM, BLOOMZ |\n",
    "| **Extrapolation** | Poor | Poor | Good | Excellent |\n",
    "| **Implementation** | Simple | Simple | Complex | Moderate |\n",
    "| **Attention Pattern** | Global | Global | Local + Global | Local + Global |\n",
    "| **Gradient Flow** | Stable | Can be unstable | Stable | Very stable |\n",
    "| **Popular Models** | Original Transformer | BERT, GPT-2 | LLaMA, GPT-J | BLOOM, BLOOMZ |\n",
    "| **Relative Position** | No | No | Yes | Yes |\n",
    "| **Train/Test Length** | Must match | Must match | Can differ | Can differ significantly |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
